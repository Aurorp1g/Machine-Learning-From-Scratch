===================================
模型调优
===================================

5.1 留出法

留出法：直接将数据切分为三个互斥的部分（也可以切分成两部分，此时训练集也是验证集），然后在训练集上训练模型，在验证集上选择模型，最后用测试集上的误差作为泛化误差的估计。

数据集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。若训练集、验证集、测试集中类别比例差别很大，则误差估计将由于训练/验证/测试数据分布的差异而产生偏差。

如：在分类任务中至少要保持样本的类别比例相似。如果从采样的角度来看到数据集的划分过程，则保留类别比例的采样方式称作“分层采样“(strafified sampling)。

即使进行了分层采样，仍然存在多种划分方式对数据集进行划分（比如排序后再划分、随机划分...）。这些不同的划分将导致不同的训练集/验证集/测试集。因此单次留出法得出的估计结果往往不够稳定可靠。

在使用留出法时，往往采用若干次随机划分、重复进行实验评估后，取平均值作为留出法的评估结果。

5.2 K 折交叉验证

K 折交叉验证法：数据随机划分为K 个互不相交且大小相同的子集，利用 K-1 个子集数据训练模型，利用余下的一个子集测试模型（一共有  种组合）。

对 K 种组合依次重复进行，获取测试误差的均值，将这个均值作为泛化误差的估计。

与留出法相似，将数据集划分为 K 个子集同样存在多种划分方式。为了减少因为样本划分不同而引入的差别， K 折交叉验证通常需要随机使用不同划分重复p 次，这 p 次 K 折交叉验证的测试误差均值作为最终的泛化误差的估计。

5.3 留一法

留一法：假设数据集中存在  个样本，令  则得到了 K 折交叉验证的一个特例。

优点：由于训练集与初始数据集相比仅仅少一个样本，因此留一法的训练数据最多。

缺点：在数据集比较大时，训练  个模型的计算量太大。

5.4 自助法

在留出法和 K 折交叉验证法中，由于保留了一部分样本用于测试，因此实际训练模型使用的训练集比初始数据集小，这必然会引入一些因为训练样本规模不同而导致的估计偏差。

留一法受训练样本规模变化的影响较小，但是计算复杂度太高。

自助法是一个以自助采样法(bootstrap sampling)为基础的比较好的解决方案。

自助采样法：给定包含  个样本的数据集  ，对它进行采样产生数据集  ：

每次随机从  中挑选一个样本，将其拷贝放入  中，然后再将该样本放回初始数据集  中（该样本下次采样时仍然可以被采到）。
重复这个过程  次，就得到了包含  个样本的数据集  。
显然，  中有些样本会在  中多次出现了；  中有些样本在  中从不出现。 中某个样本始终不被采到的概率为  。

根据极限  ，即通过自助采样，初始数据集  中约有 36.8% 的样本未出现在采样数据集 中。

将  用作训练集，  用作测试集，这样的测试结果称作包外估计out-of-bag estimate。

自助法在数据集较小时很有用。

优点：能从初始数据集中产生多个不同的训练集，这对集成学习等方法而言有很大好处。
缺点：产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此在初始数据量足够时，留出法和折交叉验证法更常用。


六、训练集、验证集、测试集
=====================================

6.1 训练集

训练集用于训练模型。理论上训练集越大越好。
6.2 验证集

大多数机器学习算法具有超参数，超参数的值无法通过学习算法拟合出来（比如正则化项的系数、控制模型容量的参数 ）。
为了解决这个问题，可以引入验证集。将训练数据分成两个不相交的子集：训练集用于学习模型，验证集用于更新超参数。
通常要求验证集足够大。如果验证集很小，那么模型的超参数可能就记住了一个小验证集里的样本，模型将对验证集严重过拟合。
验证集通常会低估泛化误差。因此当超参数优化完成后，需要通过测试集来估计泛化误差。
6.3 测试集

测试集用于评估模型的泛化误差。理论上测试集越大，则模型的泛化误差评估的越准确。

测试集中的样本一定不能是训练样本。如果将训练样本放入测试集中，则会低估泛化误差。

测试集 vs 验证集：

测试集通常用于对模型的预测能力进行评估，它提供了模型预测能力的无偏估计。

如果你不需要对模型预测能力的无偏估计，则不需要测试集。

验证集用于超参数的选择，它无法提供模型预测能力的有偏估计。

因为模型依赖于超参数，而超参数依赖于验证集。因此验证集参与了模型的构建，这意味着模型已经考虑了验证集的信息。

6.4 拆分

对于小批量数据，数据的拆分的常见比例为：

如果未设置验证集，则将数据三七分：70% 的数据用作训练集、30% 的数据用作测试集。
如果设置验证集，则将数据划分为：60% 的数据用作训练集、20%的数据用过验证集、20% 的数据用作测试集。
对于大批量数据，验证集和测试集占总数据的比例会更小。

对于百万级别的数据，其中1万条作为验证集、1万条作为测试集即可。

验证集的目的就是验证不同的超参数；测试集的目的就是比较不同的模型。

一方面它们要足够大，才足够评估超参数、模型。
另一方面，如果它们太大，则会浪费数据（验证集和训练集的数据无法用于训练）。
在k 折交叉验证中：先将所有数据拆分成 k 份，然后其中1 份作为测试集，其他k-1 份作为训练集。

这里并没有验证集来做超参数的选择。所有测试集的测试误差的均值作为模型的预测能力的一个估计。

使用k 折交叉的原因是：样本集太小。如果选择一部分数据来训练，则有两个问题：

训练数据的分布可能与真实的分布有偏离。k 折交叉让所有的数据参与训练，会使得这种偏离得到一定程度的修正。
训练数据太少，容易陷入过拟合。k 折交叉让所有数据参与训练，会一定程度上缓解过拟合。
6.5 分布不匹配

深度学习时代，经常会发生：训练集和验证集、测试集的数据分布不同。

如：训练集的数据可能是从网上下载的高清图片，测试集的数据可能是用户上传的、低像素的手机照片。

必须保证验证集、测试集的分布一致，它们都要很好的代表你的真实应用场景中的数据分布。
训练数据可以与真实应用场景中的数据分布不一致，因为最终关心的是在模型真实应用场景中的表现。
如果发生了数据不匹配问题，则可以想办法让训练集的分布更接近验证集。

一种做法是：收集更多的、分布接近验证集的数据作为训练集合。

另一种做法是：人工合成训练数据，使得它更接近验证集。

该策略有一个潜在问题：你可能只是模拟了全部数据空间中的一小部分。导致你的模型对这一小部分过拟合。

当训练集和验证集、测试集的数据分布不同时，有以下经验原则：

确保验证集和测试集的数据来自同一分布。

因为需要使用验证集来优化超参数，而优化的最终目标是希望模型在测试集上表现更好。

确保验证集和测试集能够反映未来得到的数据，或者最关注的数据。

确保数据被随机分配到验证集和测试集上。

当训练集和验证集、测试集的数据分布不同时，分析偏差和方差的方式有所不同。

如果训练集和验证集的分布一致，那么当训练误差和验证误差相差较大时，我们认为存在很大的方差问题。

如果训练集和验证集的分布不一致，那么当训练误差和验证误差相差较大时，有两种原因：

第一个原因：模型只见过训练集数据，没有见过验证集的数据导致的，是数据不匹配的问题。
第二个原因：模型本来就存在较大的方差。
为了弄清楚原因，需要将训练集再随机划分为：训练-训练集、训练-验证集。这时候，训练-训练集、训练-验证集 是同一分布的。

模型在训练-训练集 和 训练-验证集 上的误差的差距代表了模型的方差。
模型在训练-验证集 和 验证集上的误差的差距代表了数据不匹配问题的程度。