==================================
特征缩放 Feature Scaling
==================================

.. contents:: :local:


1. 为什么要做特征缩放？
=====================================================
TODO


2. 特征缩放的方法
=========================

2.1 零均值化与标准化
------------------------------------
零均值化即把每个变量的均值都变为0，方法是对每个样本的取值都减去该变量的平均值。

标准化即在零均值化的基础上更进一步，不仅把变量的均值置为0，且把方差置为1，方法是对每个样本的取值减去该变量的平均值，再除以标准差。

公式： z = (x-x.mean) / standard deviation

通过零均值化或标准化的处理，可以使得不同的特征具有相同的尺度。这样，在学习参数的时候，不同特征对参数的影响程度就一样了。

**缺点**
 - 如果变量是偏态分布的或存在极端值，则标准化后的变量分布会被压缩在一个窄小的区间内。


2.2 最大-最小值标准化
------------------------------------
变量的数值差异会影响算法的表现，而最大-最小值标准化可以将变量的值压缩到[0,1]的区间内。

公式：  X = (x-x.min) / (x.max-x.min)

**缺点**
 - 与标准化方法的缺点类似，如果变量是偏态分布的或存在极端值，则标准化后的变量分布会被压缩在一个窄小的区间内。

2.3 缩放至中位数和百分位数
------------------------------------
该方法将每个样本减去该变量的中位数，再除以四分位差（IQR）。

公式： X_scaled = (x-x.median) / IQR

IQR = 75th quantile - 25th quantile

**优点**
 - 对于偏态分布的变量，也能较好的保留原始的分布形状


2.4 Scaling to Unit Length
------------------------------------


